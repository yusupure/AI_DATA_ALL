第一课 基础了解
维数    阶     名字    例子
0-D     0     标量    S=123
1-D     1     向量    S=[1,2,3]
2-D     2     矩阵    S=[[1,2,3],[4,5,6]]
3-D     3     张量    S=[[[1,2,3],[4,5,6]]

X=tf.constant[[1.0,2.O]]
W1=tf.constant[[3.0,4.0]]
矩阵乘法
x1=1.0*w1=3.0
x2=2.0*w1=4.0
相乘后的结果进行相加=11.0
公式为：y=x1w1+x2w2

tensorflow需要创建会话才能进行计算，会话方式有两种
1.with tf.Session()as sess:          2.sess=tf.Session()
  sess.run(参数Y)                       sess.run(参数Y）
注解：
第一种方式系统自动在会话结束后自动关闭
第二种方法需要手工添加sess.close()进行会话关闭
----------------------------------------------------------------------------------------------------------------------------------------
第二课 前向传播
由于需要创建数据集来进行数据计算，TENSORFLOW有以下数据自建方法:

w=tf.Variable(tf.random_normal([2,3],stddev=2,mean=0,seed=1)
1.tf.Variable=创建一个变量的方法，但计算的时候必须创建会话进行计算，否则不会进行数据打印，sess.run(参数）
1.1 tf.random_normal=一个正态分布（[2,3]=2*3的矩阵，stddev=标准差，mean=均值，seed=随机种子）
1.2 tf.truncated_normal()=去掉偏离点的正态分布
1.3 tf.random.uniform()=平均分布
1.4 tf.zeros()=全部为0的数组 例：tf.zeros([2,3])创建为三行两列的数据[[0,0][0,0]],[[0,0][0,0]],[[0,0][0,0]]
1.5 tf.ones()=全部为1的数组 例：tf.zeros([2,3])创建为三行两列的数据[[1,1][1,1]],[1,1][1,1]],[[1,1][1,1]]
1.6 tf.fill()=全定值数组 例：tf.fill([2,3],6)|6代表设置值|下面的均值默认值为6 创建为三行两列的数据[[6,6][6,6]],[6,6][6,6]],[[6,6][6,6]]
1.7 tf.constant()=直接输出值 例：tf.constant(6),6就代表为输出值

神经网络实现原理过程
1.准备数据集，提取特征，作为输入喂给神经网络
2.搭建NN结构，从输入到输出：（先搭建计算图，在进行会话执行）
  （NN前向传播算法--》计算输出）
3.大量特征喂给NN，迭代优化NN的参数
  （NN反向传播算法--》优化参数模型）
4.使用训练好的模型预测及分类

----------------------------------------------------------------------------------------------------------------------------------------
第三课 反向传播
1.反向传播：在所有参数上用梯度下降，使NN模型在训练数据损失函数减少
损失函数（梯度下降法）
loss=预测值-已知答案的差距（PD-y）
默认公式：
Loss=平均值(平方（预测值/已知答案））
例子
loss=tf.reduce_mean(tf.square(预测值/已知答案))
reduce_mean=计算平均值
tf.square=计算两个值相除后的平方

2.方向训练：用于减少Loss值为优先目标
梯度下降法优化器
tf.train.GradientDescentOptimizer(小于1的损失值).minimize(loss)
tf.train.MomentumOptimizer(小于1的损失值).minimize(loss)
tf.train.AdamOptimizer(小于1的损失值).minimize(loss)

3.常用激活函数
3.1.tf.nn.relu
  =(0 x<=0),(x x>=0)
  
3.2.tf.nn.sigmiod
    f(x)=1/1+(e-x)

3.3.tf.nn.tanh
  f(x)=1-(e-2x)/1+(e-2x)
  
4.NN复杂程度：多用NN层数和NN参数个数表示
  层数=隐藏层的层数+1个输出层
  总参数=总w+总b
  
5.图解结构计算
O O 
O O O
O O O
  O
A B C
1层图解
3个A*4个神经元+4（4为偏至项B）
第二层
4个神经元*2个输出层+2（2为偏执项B）
最终结果为:一层的总和+2层的总和=总输出
输入层计算:a=(x1(0.7)*w1(0.2)+x2(0.5)*w2(0.3))*w2(0.1)
输出层计算:a(总和)*w1(0.1)为第一组数据。
----------------------------------------------------------------------------------------------------------------------------------------
第四课 优化神经网络
1.损失函数Loss
2.学习率
3.滑动平均
A:损失函数
预测值 y 与已知答案 pd 的差距(第一个预测结果PD-原始数据Y的结果）
NN优化
1.均方误差
2.自定损失函数
3.交叉熵
以上均为获取Loss最小值为目的
均方误差案例
tf.reduce_mean(tf.square(pd-y）
/////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////
2.自定义损失函数
如果预测的商品销量，预测多了损失成本，少了损失利润
若成本不等于利润，则mse产生的loss无法利益最大化

自定义函数公式:[loss=预测值PD，已有答案的Y数值]=Ef(pd-y)
f为激活函数
      
f(pd-y)=:
profit*(y-pd) 预测值大于 已知答案，预测Y少了，损失利润
cost*(pd-y) 已知答案 小于 预测值  损失成本

案例
loss=tf.reduce_sum(tf.where(tf.greater(pd,y),(pd-y)*cost,(y-pd)*profit))

/////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////
3.交叉熵：表示两个概率分布之间的距离
H(y,pd)=Ey*log pd

2分类已知答案Y=（1，0）Y1预测结果为（0.6，0.4）Y2(0.8,0.2)
公式为:
H1((1,0),(0.6,0.4)=-((1*LOG-0.6+0*LOG*0.4)=(0.222+0)
H2((1,0),(0.8,0.2)=-((1*LOG-0.8+0*LOG*0.2)=(0.097+0)

执行方法：
ce=tf.reduce_mean(y*tf.log(tf.clip.by_value(pd,1e-12,10)
防止出Log=0，限制小于10的-12次方 大于1.0等于1.0

替换以上方法
当N分类的N个输出(y1,y2,yn)通过softmax()函数
遍满足概率分布
反Ax p(x=X)E[1,0]且p(x=X)=1
ce=tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=pd,labels=y))

----------------------------------------------------------------------------------------------------------------------------------------
第五课 学习率
学习率：每次参数更新的幅度（learning_rate）
wn+1=wn-learning_rate▲
wn+1:更新后的值
wn:当前参数
learning_rate:学习率
▲:损失函数的梯度即为Loss值

eg损失函数:loss=(w+1)的平方 梯度▲=a*loss/aw=2w+2
例子
1参数为W:5      5-0.2*(2*5+2)=2.6
2参数为W:2.6    2.6-0.2*(2*2.6+2)=1.16
3参数为W:1.16   1.16-0.2*(2*1.16+2)=0.296

学习率设置多少为合适？
loss=tf.square(w+1)
学习率为=1
之后学习率为0.0001
大了震荡不收敛，小了收敛速度慢

自动化损失率控制
指数衰减学习率
案例：
LEARNING_RATE_BASE=1      学习率初始值
LEARNING_RATE_STEP=0.99   多少轮更新一次学习率
LEARNING_RATE_DECAY=1     学习率衰减率[0,1]
GLOBAT_STEP=tf.Variable(0,trainable=FALSE)   trainable注明此参数不参与计算(一般为batch_size/样本数）运行轮数)
执行例子：
leaning_rate=tf.train.exponetial_decay(学习率初始值，运行轮数，多少轮更新一次学习率，学习率衰减率，staircase=True/false)
在train_step=.GradientDescentOptimizer(leaning_rate).minimize(loss,GLOBAT_STEP)

----------------------------------------------------------------------------------------------------------------------------------------
第六课 滑动平均
滑动平均(影子值：记录每次参数一段时间内过往的值的平均）
增加模型泛化率
公式：
影子=衰减率*影子+(1-衰减率）*参数（影子初始=参数初始W1）
衰减率=min{moving_average_decay,1+轮数/10+轮数）
moving_average_decay=0.99默认为这个最大值

执行案例:
moving_average_decay=0.99
w1=0
globat_step=0

W1=1
W1的滑动平均值=min(0.99,1/10)*0+(1-min(0.99,1/10)*1=0.9
若轮数修改为100 W1=10
W1的滑动平均值=min(0.99,101/110)*0.9+(1-min(0.99,101/110)*10=1.644
W1的滑动平均值=min(0.99,101/110)*1.644+(1-min(0.99,101/110)*10=2.328

使用方法
ema=tf.train.ExponentialMovingAverage(moving_average_decay,globat_step)
ema_op=ema.apply([w1])
ema_op=ema.apply(tf.trainable_variables())

ema.arerage(W1）用于查询滑动平均值
sess.run(tf.assign(w1,10))更新值的方法
sess.run(tf.assign(globat_step,100))
打印方式:
sess.run(ema_op)影子值



